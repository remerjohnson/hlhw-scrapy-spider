{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scrapy to make a spider for HLHW Workshop events\n",
    "Every year, a page of the upcoming HLHW events is posted on the Library web site. The events for the [2018-2019](https://libraries.ucsd.edu/visit/library-workshops/holocaust-living-history-workshop/events/2018-2019.html) season are up. Can we make a bot that will scrape the pages so we don't have to manually key in data? Let's try using the python program [Scrapy](https://doc.scrapy.org/en/latest)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy set up\n",
    "The tutorial for Scrapy says we first have to run some commands to set up the project: \n",
    "```\n",
    "$ scrapy startproject hlhw\n",
    "```\n",
    "\n",
    "This returned the message: \n",
    "```\n",
    "You can start your first spider with:\n",
    "    cd hlhw\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "\n",
    "But going through the tutorial, we will move on to making our spider, within the project directory this command created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the spider \n",
    "\n",
    "We will use the turotial's spider as a template, but plug in our relevant info, and save as a python (.py) file: `hlhw_events.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class EventsSpider(scrapy.Spider):\n",
    "    name = \"events\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = 'https://libraries.ucsd.edu/visit/library-workshops/holocaust-living-history-workshop/events/2018-2019.html'\n",
    "        for u in url:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'hlhw-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to parse \n",
    "This command dumped an .html file in our project, but we need to learn how to parse using the Scrapy shell interface in order to make our spider really smart. The tutorial says to start the Scrapy shell with \n",
    "```\n",
    "$ scrapy shell 'https://libraries.ucsd.edu/visit/library-workshops/holocaust-living-history-workshop/events/2018-2019.html'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we're in the shell, it has a REPL environment (not unlike Jupyter!) where we can run commands and get output. \n",
    "\n",
    "For example, we run: \n",
    "```\n",
    "In [1]: response.css('title')\n",
    "Out[1]: [<Selector xpath='descendant-or-self::title' data='<title>2018 - 2019 Events</title>'>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes a Selector object we can extract text from:  \n",
    "```\n",
    "In [2]: response.css('title::text').extract()\n",
    "Out[2]: ['2018 - 2019 Events']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big thing we'd like to do is capture the description of the events. Luckily, these are only captured in the paragraph `<p>` tags. Unfortunately, there's other tags sometimes nested within those tags (most commonly italics `<i>` tags). We can make the selector a little greedy by changing the XPath request: \n",
    "\n",
    "```\n",
    "In [23]: response.xpath('//p')\n",
    "Out[23]: \n",
    "[<Selector xpath='//p' data='<p>Paneriai is the Lithuanian name for P'>,\n",
    " <Selector xpath='//p' data='<p>“I live in crazy times,” Anne Frank w'>,\n",
    " <Selector xpath='//p' data='<p>Despite the explosive growth of Holoc'>,\n",
    " <Selector xpath='//p' data='<p>The fate of Bulgarian Jewry during Wo'>,\n",
    " <Selector xpath='//p' data='<p>It is a common misconception that Jew'>,\n",
    " <Selector xpath='//p' data='<p>The suite of international convention'>,\n",
    " <Selector xpath='//p' data='<p>Louis “Lubo” Pechi was born in the Cr'>,\n",
    " <Selector xpath='//p' data='<p>Every once in a while a book comes al'>]\n",
    "```\n",
    "\n",
    "Now, we extract the text from those selectors: \n",
    "```\n",
    "In [24]: for p in response.xpath('//p'):\n",
    "    ...:     print(p.extract())\n",
    "    ...:     \n",
    "<p>Paneriai is the Lithuanian name for Ponar (Ponary in Polish), the site of one of the worst massacres of Jews during World War II. For Barbara Michelman Panieriai is a landscape of loss and silence – a silence exemplified by her father who was born there. Though he escaped the slaughter, he was nevertheless broken by its haunting dimensions. In her solo-exhibition <i>Past is Prologue</i>, a series of photo montages with text, ...\n",
    "```\n",
    "\n",
    "This works well. Now we just have to loop through these in order to get them into an output like JSON/csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
